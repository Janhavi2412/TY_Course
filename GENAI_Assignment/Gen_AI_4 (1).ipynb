{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNOHhU/7D/RSOIi2PgKyAH2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from transformers import AutoModelForQuestionAnswering, AutoTokenizer\n","import torch\n","\n","class SimpleQABot:\n","    def __init__(self, model_name=\"bert-large-uncased-whole-word-masking-finetuned-squad\"):\n","        \"\"\"\n","        Initialize the QA bot with a pre-trained model\n","        Args:\n","            model_name (str): Name of the pre-trained model to use\n","        \"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModelForQuestionAnswering.from_pretrained(model_name)\n","\n","    def answer_question(self, context, question):\n","        \"\"\"\n","        Answer a question based on the given context\n","        Args:\n","            context (str): The text passage to search for answers\n","            question (str): The question to answer\n","        Returns:\n","            dict: Contains the answer text and confidence score\n","        \"\"\"\n","        # Tokenize input\n","        inputs = self.tokenizer.encode_plus(\n","            question,\n","            context,\n","            add_special_tokens=True,\n","            return_tensors=\"pt\"\n","        )\n","\n","        # Get model output\n","        input_ids = inputs[\"input_ids\"].tolist()[0]\n","        outputs = self.model(**inputs)\n","        answer_start = torch.argmax(outputs.start_logits)\n","        answer_end = torch.argmax(outputs.end_logits)\n","\n","        # Convert tokens to answer text\n","        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n","        answer = tokens[answer_start]\n","\n","        # Combine subword tokens\n","        for i in range(answer_start + 1, answer_end + 1):\n","            if tokens[i].startswith(\"##\"):\n","                answer += tokens[i][2:]\n","            else:\n","                answer += \" \" + tokens[i]\n","\n","        # Calculate confidence score\n","        start_score = torch.max(torch.softmax(outputs.start_logits, dim=1)).item()\n","        end_score = torch.max(torch.softmax(outputs.end_logits, dim=1)).item()\n","        confidence = (start_score + end_score) / 2\n","\n","        return {\n","            \"answer\": answer,\n","            \"confidence\": confidence\n","        }\n","\n","def main():\n","    # Initialize the chatbot\n","    bot = SimpleQABot()\n","\n","    print(\"Simple QA Chatbot (type 'quit' to exit)\")\n","    print(\"First, please provide some context text for the bot to use.\")\n","\n","    context = input(\"\\nContext: \").strip()\n","\n","    while True:\n","        # Get question from user\n","        question = input(\"\\nQuestion: \").strip()\n","\n","        if question.lower() == 'quit':\n","            break\n","\n","        # Get and display answer\n","        try:\n","            result = bot.answer_question(context, question)\n","            print(f\"\\nAnswer: {result['answer']}\")\n","            print(f\"Confidence: {result['confidence']:.2%}\")\n","        except Exception as e:\n","            print(f\"Error: {str(e)}\")\n","\n","        print(\"\\n\" + \"-\"*50)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H39CwWoPedoh","outputId":"f00bf012-6f05-4fe3-d99d-2c610a6ebbc5","executionInfo":{"status":"ok","timestamp":1739773242997,"user_tz":-330,"elapsed":31729,"user":{"displayName":"JANHAVI GUNDAWAR","userId":"12284019592857865944"}}},"execution_count":9,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n","- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]},{"name":"stdout","output_type":"stream","text":["Simple QA Chatbot (type 'quit' to exit)\n","First, please provide some context text for the bot to use.\n","\n","Answer: a type of artificial intelligence ( ai )\n","Confidence: 55.57%\n","\n","--------------------------------------------------\n","\n","Question: quit\n"]}]}]}
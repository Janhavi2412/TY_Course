{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNuXpW1zk5JN29euLmc9dm+"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"daQ0wudgh3cN"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import re\n","import string\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","import pickle"]},{"cell_type":"code","source":["# Download required NLTK resources\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('punkt_tab') # Download the punkt_tab resource"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8MW_KC4CiIwh","executionInfo":{"status":"ok","timestamp":1738296607093,"user_tz":-330,"elapsed":703,"user":{"displayName":"JANHAVI GUNDAWAR","userId":"12284019592857865944"}},"outputId":"1699d67d-290b-4627-e640-b71745bb5ada"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["# Sample dataset\n","data = {\n","    \"Text\": [\n","        \"Natural Language Processing is a branch of AI.\",\n","        \"AI models are trained using data.\",\n","        \"Processing natural text is challenging!\",\n","        \"Word embeddings capture meaning in text.\"\n","    ],\n","    \"Category\": [\"NLP\", \"AI\", \"NLP\", \"ML\"]  # Example labels\n","}"],"metadata":{"id":"5xcZRhoSiN9K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert to DataFrame\n","df = pd.DataFrame(data)"],"metadata":{"id":"GBqPfEHeiS_r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 1: Text Cleaning Function\n","def clean_text(text):\n","    text = text.lower()  # Lowercase\n","    text = re.sub(r'\\d+', '', text)  # Remove numbers\n","    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n","    return text\n","\n","df[\"Cleaned_Text\"] = df[\"Text\"].apply(clean_text)"],"metadata":{"id":"GgNPPVR3iXyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 2: Tokenization, Stopword Removal, and Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","stop_words = set(stopwords.words(\"english\"))\n","\n","def preprocess_text(text):\n","    tokens = word_tokenize(text)  # Tokenization\n","    tokens = [word for word in tokens if word not in stop_words]  # Remove stopwords\n","    tokens = [lemmatizer.lemmatize(word) for word in tokens]  # Lemmatization\n","    return \" \".join(tokens)\n","\n","df[\"Processed_Text\"] = df[\"Cleaned_Text\"].apply(preprocess_text)"],"metadata":{"id":"XGBngHaLicXj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Step 3: Label Encoding\n","label_encoder = LabelEncoder()\n","df[\"Label\"] = label_encoder.fit_transform(df[\"Category\"])"],"metadata":{"id":"-Saqn1_SjbHz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Step 4: TF-IDF Representation\n","tfidf_vectorizer = TfidfVectorizer()\n","tfidf_matrix = tfidf_vectorizer.fit_transform(df[\"Processed_Text\"])"],"metadata":{"id":"kWApXTuxjd2I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Convert TF-IDF matrix to DataFrame\n","tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())"],"metadata":{"id":"ZesDjS55jn6w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#  Step 5: Save Outputs\n","df.to_csv(\"processed_data.csv\", index=False)  # Save preprocessed text\n","tfidf_df.to_csv(\"tfidf_representation.csv\", index=False)  # Save TF-IDF output"],"metadata":{"id":"yKMWPRyyjr7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save label encoder for later use\n","with open(\"label_encoder.pkl\", \"wb\") as f:\n","    pickle.dump(label_encoder, f)"],"metadata":{"id":"TftvkqtAjwKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Save TF-IDF vectorizer for later use\n","with open(\"tfidf_vectorizer.pkl\", \"wb\") as f:\n","    pickle.dump(tfidf_vectorizer, f)\n","\n","print(\" Text processing completed and output saved!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k9c9P4NMj2_H","executionInfo":{"status":"ok","timestamp":1738296764217,"user_tz":-330,"elapsed":6,"user":{"displayName":"JANHAVI GUNDAWAR","userId":"12284019592857865944"}},"outputId":"b4e94e51-b417-4c89-c756-b64ef34a1ef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" Text processing completed and output saved!\n"]}]}]}